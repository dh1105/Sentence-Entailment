{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentence Entailment ALBERT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dh1105/Sentence-Entailment/blob/main/Sentence_Entailment_ALBERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9YGhQEYnj7DD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5a6f775-d13c-4297-d636-24aa2bdec927"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80g_Etp2kNeZ",
        "outputId": "7de1d66a-3383-4afe-dc54-4425973865dc"
      },
      "source": [
        "!pip install transformers\r\n",
        "!pip install sentencepiece\r\n",
        "# !pip install cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.7-cp36-cp36m-linux_x86_64.whl"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (4.1.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.8)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tokenizers==0.9.4 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.9.4)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (0.1.94)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qS9dZ8F3j9J8"
      },
      "source": [
        "import pandas as pd\r\n",
        "import re\r\n",
        "import torch\r\n",
        "# import torch_xla\r\n",
        "# import torch_xla.core.xla_model as xm\r\n",
        "from torch.utils.data import Dataset, TensorDataset, DataLoader, SequentialSampler, RandomSampler\r\n",
        "from torch.nn.utils.rnn import pad_sequence\r\n",
        "# from keras.preprocessing.sequence import pad_sequences\r\n",
        "import pickle\r\n",
        "import os\r\n",
        "from transformers import AlbertTokenizer\r\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWq_IaW1GdfI"
      },
      "source": [
        "# device = xm.xla_device()\r\n",
        "# print(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WkpfPDS1Z8a8",
        "outputId": "fdc8e9db-3d1e-47ab-f255-aa4dcf272d61"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\r\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDulyi-rXHQy"
      },
      "source": [
        "train_df = pd.read_csv('/content/drive/My Drive/Sentence entailment/train_data.csv')[:100000]\r\n",
        "val_df   = pd.read_csv('/content/drive/My Drive/Sentence entailment/val_data.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3VB-nH4XcGj"
      },
      "source": [
        "train_df = train_df.dropna()\r\n",
        "val_df = val_df.dropna()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMTQb1iMF54W"
      },
      "source": [
        "train_df['sentence1'] = train_df['sentence1'].astype(str)\r\n",
        "train_df['sentence2'] = train_df['sentence2'].astype(str)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMzPNmtpF8yg"
      },
      "source": [
        "val_df['sentence1'] = val_df['sentence1'].astype(str)\r\n",
        "val_df['sentence2'] = val_df['sentence2'].astype(str)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWHuqSmJYEHT"
      },
      "source": [
        "train_df = train_df[(train_df['sentence1'].str.split().str.len() > 0) & (train_df['sentence2'].str.split().str.len() > 0)]\r\n",
        "val_df = val_df[(val_df['sentence1'].str.split().str.len() > 0) & (val_df['sentence2'].str.split().str.len() > 0)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "id": "yYV1ofZTYpHC",
        "outputId": "1253e09f-5e88-42e7-ef4b-83ac5a474216"
      },
      "source": [
        "train_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>gold_label</th>\n",
              "      <th>sentence1</th>\n",
              "      <th>sentence2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>neutral</td>\n",
              "      <td>Conceptually cream skimming has two basic dime...</td>\n",
              "      <td>Product and geography are what make cream skim...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>entailment</td>\n",
              "      <td>you know during the season and i guess at at y...</td>\n",
              "      <td>You lose the things to the following level if ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>entailment</td>\n",
              "      <td>One of our number will carry out your instruct...</td>\n",
              "      <td>A member of my team will execute your orders w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>entailment</td>\n",
              "      <td>How do you know? All this is their information...</td>\n",
              "      <td>This information belongs to them.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>neutral</td>\n",
              "      <td>yeah i tell you what though if you go price so...</td>\n",
              "      <td>The tennis shoes have a range of prices.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99995</th>\n",
              "      <td>99995</td>\n",
              "      <td>neutral</td>\n",
              "      <td>She'd probably ask you the way to the Moat Hou...</td>\n",
              "      <td>Tuppence went missing whilst looking for the m...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99996</th>\n",
              "      <td>99996</td>\n",
              "      <td>entailment</td>\n",
              "      <td>When reporters asked whether Clinton should re...</td>\n",
              "      <td>Personal destruction politics should not decid...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99997</th>\n",
              "      <td>99997</td>\n",
              "      <td>neutral</td>\n",
              "      <td>For the hikers, a bus goes to a point quite cl...</td>\n",
              "      <td>At the summit, the view is said to be the best...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99998</th>\n",
              "      <td>99998</td>\n",
              "      <td>entailment</td>\n",
              "      <td>The capital of the Gododdin was Din Eidyn (the...</td>\n",
              "      <td>Edinburgh derived its name from Din Eidyn, the...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99999</th>\n",
              "      <td>99999</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>In any event, Gore dropped out of the race sho...</td>\n",
              "      <td>In each one of the events, Bush quit the race ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>99985 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       Unnamed: 0  ...                                          sentence2\n",
              "0               0  ...  Product and geography are what make cream skim...\n",
              "1               1  ...  You lose the things to the following level if ...\n",
              "2               2  ...  A member of my team will execute your orders w...\n",
              "3               3  ...                  This information belongs to them.\n",
              "4               4  ...           The tennis shoes have a range of prices.\n",
              "...           ...  ...                                                ...\n",
              "99995       99995  ...  Tuppence went missing whilst looking for the m...\n",
              "99996       99996  ...  Personal destruction politics should not decid...\n",
              "99997       99997  ...  At the summit, the view is said to be the best...\n",
              "99998       99998  ...  Edinburgh derived its name from Din Eidyn, the...\n",
              "99999       99999  ...  In each one of the events, Bush quit the race ...\n",
              "\n",
              "[99985 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "id": "gQLZTFJjYqLL",
        "outputId": "57526d21-f8c0-4608-93c3-6d4667afe692"
      },
      "source": [
        "val_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>gold_label</th>\n",
              "      <th>sentence1</th>\n",
              "      <th>sentence2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>neutral</td>\n",
              "      <td>The new rights are nice enough</td>\n",
              "      <td>Everyone really likes the newest benefits</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>This site includes a list of all award winners...</td>\n",
              "      <td>The Government Executive articles housed on th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>entailment</td>\n",
              "      <td>uh i don't know i i have mixed emotions about ...</td>\n",
              "      <td>I like him for the most part, but would still ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>yeah i i think my favorite restaurant is alway...</td>\n",
              "      <td>My favorite restaurants are always at least a ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>i don't know um do you do a lot of camping</td>\n",
              "      <td>I know exactly.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9810</th>\n",
              "      <td>9995</td>\n",
              "      <td>neutral</td>\n",
              "      <td>Since 1998, LSC has initiated and overseen sig...</td>\n",
              "      <td>LSC has been focusing on improving it's state ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9811</th>\n",
              "      <td>9996</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>Eighty percent of pagers in the United States ...</td>\n",
              "      <td>Pagers in the United States were unaffected by...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9812</th>\n",
              "      <td>9997</td>\n",
              "      <td>entailment</td>\n",
              "      <td>Finally, the FDA will conduct workshops, issue...</td>\n",
              "      <td>The FDA is set to conduct workshops.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9813</th>\n",
              "      <td>9998</td>\n",
              "      <td>entailment</td>\n",
              "      <td>Cirque du Soleil's The latest from the acclaim...</td>\n",
              "      <td>Cirque du Soleil is an international troupe.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9814</th>\n",
              "      <td>9999</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>i'll listen  and agree with what i think sound...</td>\n",
              "      <td>I wont even bother listening.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>9815 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      Unnamed: 0  ...                                          sentence2\n",
              "0              0  ...         Everyone really likes the newest benefits \n",
              "1              1  ...  The Government Executive articles housed on th...\n",
              "2              2  ...  I like him for the most part, but would still ...\n",
              "3              3  ...  My favorite restaurants are always at least a ...\n",
              "4              4  ...                                    I know exactly.\n",
              "...          ...  ...                                                ...\n",
              "9810        9995  ...  LSC has been focusing on improving it's state ...\n",
              "9811        9996  ...  Pagers in the United States were unaffected by...\n",
              "9812        9997  ...              The FDA is set to conduct workshops. \n",
              "9813        9998  ...       Cirque du Soleil is an international troupe.\n",
              "9814        9999  ...                      I wont even bother listening.\n",
              "\n",
              "[9815 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHpMlxVYZW9x"
      },
      "source": [
        "import torch\r\n",
        "from torch.utils.data import Dataset, TensorDataset, DataLoader\r\n",
        "from torch.nn.utils.rnn import pad_sequence\r\n",
        "import pickle\r\n",
        "import os\r\n",
        "from transformers import AlbertTokenizer\r\n",
        "\r\n",
        "class MNLIDataAlbert(Dataset):\r\n",
        "\r\n",
        "  def __init__(self, train_df, val_df):\r\n",
        "    self.label_dict = {'entailment': 0, 'contradiction': 1, 'neutral': 2}\r\n",
        "\r\n",
        "    self.train_df = train_df\r\n",
        "    self.val_df = val_df\r\n",
        "\r\n",
        "    self.base_path = '/content/'\r\n",
        "    self.tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2', do_lower_case=True)\r\n",
        "    self.train_data = None\r\n",
        "    self.val_data = None\r\n",
        "    self.init_data()\r\n",
        "\r\n",
        "  def init_data(self):\r\n",
        "    # Saving takes too much RAM\r\n",
        "    #\r\n",
        "    # if os.path.exists(os.path.join(self.base_path, 'train_data.pkl')):\r\n",
        "    #   print(\"Found training data\")\r\n",
        "    #   with open(os.path.join(self.base_path, 'train_data.pkl'), 'rb') as f:\r\n",
        "    #     self.train_data = pickle.load(f)\r\n",
        "    # else:\r\n",
        "    #   self.train_data = self.load_data(self.train_df)\r\n",
        "    #   with open(os.path.join(self.base_path, 'train_data.pkl'), 'wb') as f:\r\n",
        "    #     pickle.dump(self.train_data, f)\r\n",
        "    # if os.path.exists(os.path.join(self.base_path, 'val_data.pkl')):\r\n",
        "    #   print(\"Found val data\")\r\n",
        "    #   with open(os.path.join(self.base_path, 'val_data.pkl'), 'rb') as f:\r\n",
        "    #     self.val_data = pickle.load(f)\r\n",
        "    # else:\r\n",
        "    #   self.val_data = self.load_data(self.val_df)\r\n",
        "    #   with open(os.path.join(self.base_path, 'val_data.pkl'), 'wb') as f:\r\n",
        "    #     pickle.dump(self.val_data, f)\r\n",
        "    self.train_data = self.load_data(self.train_df)\r\n",
        "    self.val_data = self.load_data(self.val_df)\r\n",
        "\r\n",
        "  def load_data(self, df):\r\n",
        "    MAX_LEN = 512\r\n",
        "    token_ids = []\r\n",
        "    mask_ids = []\r\n",
        "    seg_ids = []\r\n",
        "    y = []\r\n",
        "\r\n",
        "    premise_list = df['sentence1'].to_list()\r\n",
        "    hypothesis_list = df['sentence2'].to_list()\r\n",
        "    label_list = df['gold_label'].to_list()\r\n",
        "\r\n",
        "    for (premise, hypothesis, label) in zip(premise_list, hypothesis_list, label_list):\r\n",
        "      premise_id = self.tokenizer.encode(premise, add_special_tokens = False)\r\n",
        "      hypothesis_id = self.tokenizer.encode(hypothesis, add_special_tokens = False)\r\n",
        "      pair_token_ids = [self.tokenizer.cls_token_id] + premise_id + [self.tokenizer.sep_token_id] + hypothesis_id + [self.tokenizer.sep_token_id]\r\n",
        "      premise_len = len(premise_id)\r\n",
        "      hypothesis_len = len(hypothesis_id)\r\n",
        "\r\n",
        "      segment_ids = torch.tensor([0] * (premise_len + 2) + [1] * (hypothesis_len + 1))  # sentence 0 and sentence 1\r\n",
        "      attention_mask_ids = torch.tensor([1] * (premise_len + hypothesis_len + 3))  # mask padded values\r\n",
        "\r\n",
        "      token_ids.append(torch.tensor(pair_token_ids))\r\n",
        "      seg_ids.append(segment_ids)\r\n",
        "      mask_ids.append(attention_mask_ids)\r\n",
        "      y.append(self.label_dict[label])\r\n",
        "    \r\n",
        "    token_ids = pad_sequence(token_ids, batch_first=True)\r\n",
        "    mask_ids = pad_sequence(mask_ids, batch_first=True)\r\n",
        "    seg_ids = pad_sequence(seg_ids, batch_first=True)\r\n",
        "    y = torch.tensor(y)\r\n",
        "    dataset = TensorDataset(token_ids, mask_ids, seg_ids, y)\r\n",
        "    print(len(dataset))\r\n",
        "    return dataset\r\n",
        "\r\n",
        "  def get_data_loaders(self, batch_size=32, shuffle=True):\r\n",
        "    train_loader = DataLoader(\r\n",
        "      self.train_data,\r\n",
        "      shuffle=shuffle,\r\n",
        "      batch_size=batch_size\r\n",
        "    )\r\n",
        "\r\n",
        "    val_loader = DataLoader(\r\n",
        "      self.val_data,\r\n",
        "      shuffle=shuffle,\r\n",
        "      batch_size=batch_size\r\n",
        "    )\r\n",
        "\r\n",
        "    return train_loader, val_loader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usRpEcPWe7re",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1300b6be-55c8-40fd-d4bb-bad1739834ad"
      },
      "source": [
        "mnli_dataset = MNLIDataAlbert(train_df, val_df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "99985\n",
            "9815\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "faJUFZc-hRrD"
      },
      "source": [
        "train_loader, val_loader = mnli_dataset.get_data_loaders(batch_size=16)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qOdc4Cs2DEjt",
        "outputId": "c7e9663e-1ae0-4fa9-ffc9-6b5fcc69b306"
      },
      "source": [
        "from transformers import AlbertForSequenceClassification, AdamW\r\n",
        "\r\n",
        "model = AlbertForSequenceClassification.from_pretrained(\"albert-base-v2\", num_labels=3)\r\n",
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertForSequenceClassification: ['predictions.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.decoder.bias']\n",
            "- This IS expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AlbertForSequenceClassification(\n",
              "  (albert): AlbertModel(\n",
              "    (embeddings): AlbertEmbeddings(\n",
              "      (word_embeddings): Embedding(30000, 128, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 128)\n",
              "      (token_type_embeddings): Embedding(2, 128)\n",
              "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0, inplace=False)\n",
              "    )\n",
              "    (encoder): AlbertTransformer(\n",
              "      (embedding_hidden_mapping_in): Linear(in_features=128, out_features=768, bias=True)\n",
              "      (albert_layer_groups): ModuleList(\n",
              "        (0): AlbertLayerGroup(\n",
              "          (albert_layers): ModuleList(\n",
              "            (0): AlbertLayer(\n",
              "              (full_layer_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (attention): AlbertAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (attention_dropout): Dropout(p=0, inplace=False)\n",
              "                (output_dropout): Dropout(p=0, inplace=False)\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              )\n",
              "              (ffn): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (ffn_output): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (pooler_activation): Tanh()\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxzpENXlEh-u"
      },
      "source": [
        "param_optimizer = list(model.named_parameters())\r\n",
        "no_decay = ['bias', 'gamma', 'beta']\r\n",
        "optimizer_grouped_parameters = [\r\n",
        "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\r\n",
        "     'weight_decay_rate': 0.01},\r\n",
        "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\r\n",
        "     'weight_decay_rate': 0.0}\r\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "is1TqwTREid9"
      },
      "source": [
        "# This variable contains all of the hyperparemeter information our training loop needs\r\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5, correct_bias=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62VHc6kPVZhx"
      },
      "source": [
        "# for name, param in model.named_parameters():                \r\n",
        "#     if name.startswith('albert'):\r\n",
        "#         param.requires_grad = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FaW8zvjdUCLP",
        "outputId": "f02bab3e-5094-49a4-f8c0-6520adf9a1e0"
      },
      "source": [
        "def count_parameters(model):\r\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\r\n",
        "\r\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 11,685,891 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vKtx05rXHLI"
      },
      "source": [
        "def multi_acc(y_pred, y_test):\r\n",
        "  acc = (torch.log_softmax(y_pred, dim=1).argmax(dim=1) == y_test).sum().float() / float(y_test.size(0))\r\n",
        "  return acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TsJ3tm74XtKF"
      },
      "source": [
        "import time\r\n",
        "\r\n",
        "EPOCHS = 5\r\n",
        "\r\n",
        "def train(model, train_loader, val_loader, optimizer):  \r\n",
        "  total_step = len(train_loader)\r\n",
        "\r\n",
        "  for epoch in range(EPOCHS):\r\n",
        "    start = time.time()\r\n",
        "    model.train()\r\n",
        "    total_train_loss = 0\r\n",
        "    total_train_acc  = 0\r\n",
        "    for batch_idx, (pair_token_ids, mask_ids, seg_ids, y) in enumerate(train_loader):\r\n",
        "      optimizer.zero_grad()\r\n",
        "      pair_token_ids = pair_token_ids.to(device)\r\n",
        "      mask_ids = mask_ids.to(device)\r\n",
        "      seg_ids = seg_ids.to(device)\r\n",
        "      labels = y.to(device)\r\n",
        "      # prediction = model(pair_token_ids, mask_ids, seg_ids)\r\n",
        "      loss, prediction = model(pair_token_ids, \r\n",
        "                             token_type_ids=seg_ids, \r\n",
        "                             attention_mask=mask_ids, \r\n",
        "                             labels=labels).values()\r\n",
        "\r\n",
        "      # loss = criterion(prediction, labels)\r\n",
        "      acc = multi_acc(prediction, labels)\r\n",
        "\r\n",
        "      loss.backward()\r\n",
        "      optimizer.step()\r\n",
        "      \r\n",
        "      total_train_loss += loss.item()\r\n",
        "      total_train_acc  += acc.item()\r\n",
        "\r\n",
        "    train_acc  = total_train_acc/len(train_loader)\r\n",
        "    train_loss = total_train_loss/len(train_loader)\r\n",
        "    model.eval()\r\n",
        "    total_val_acc  = 0\r\n",
        "    total_val_loss = 0\r\n",
        "    with torch.no_grad():\r\n",
        "      for batch_idx, (pair_token_ids, mask_ids, seg_ids, y) in enumerate(val_loader):\r\n",
        "        optimizer.zero_grad()\r\n",
        "        pair_token_ids = pair_token_ids.to(device)\r\n",
        "        mask_ids = mask_ids.to(device)\r\n",
        "        seg_ids = seg_ids.to(device)\r\n",
        "        labels = y.to(device)\r\n",
        "\r\n",
        "        # prediction = model(pair_token_ids, mask_ids, seg_ids)\r\n",
        "        loss, prediction = model(pair_token_ids, \r\n",
        "                             token_type_ids=seg_ids, \r\n",
        "                             attention_mask=mask_ids, \r\n",
        "                             labels=labels).values()\r\n",
        "        \r\n",
        "        # loss = criterion(prediction, labels)\r\n",
        "        acc = multi_acc(prediction, labels)\r\n",
        "\r\n",
        "        total_val_loss += loss.item()\r\n",
        "        total_val_acc  += acc.item()\r\n",
        "\r\n",
        "    val_acc  = total_val_acc/len(val_loader)\r\n",
        "    val_loss = total_val_loss/len(val_loader)\r\n",
        "    end = time.time()\r\n",
        "    hours, rem = divmod(end-start, 3600)\r\n",
        "    minutes, seconds = divmod(rem, 60)\r\n",
        "\r\n",
        "    print(f'Epoch {epoch+1}: train_loss: {train_loss:.4f} train_acc: {train_acc:.4f} | val_loss: {val_loss:.4f} val_acc: {val_acc:.4f}')\r\n",
        "    print(\"{:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drr2Y2eRY7op",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36d6925c-56ed-4c1e-9eb4-839a63b050c7"
      },
      "source": [
        "train(model, train_loader, val_loader, optimizer)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1: train_loss: 1.1103 train_acc: 0.3380 | val_loss: 1.0990 val_acc: 0.3272\n",
            "01:43:55.22\n",
            "Epoch 2: train_loss: 1.1042 train_acc: 0.3401 | val_loss: 1.1037 val_acc: 0.3274\n",
            "01:43:54.28\n",
            "Epoch 3: train_loss: 0.9785 train_acc: 0.4922 | val_loss: 0.8610 val_acc: 0.6231\n",
            "01:44:05.21\n",
            "Epoch 4: train_loss: 0.7669 train_acc: 0.6731 | val_loss: 0.7420 val_acc: 0.6700\n",
            "01:44:06.79\n",
            "Epoch 5: train_loss: 0.6356 train_acc: 0.7405 | val_loss: 0.7003 val_acc: 0.7110\n",
            "01:44:13.44\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}